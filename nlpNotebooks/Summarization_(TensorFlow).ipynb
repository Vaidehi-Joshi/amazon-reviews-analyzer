{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbV7t5LJkKOU"
      },
      "source": [
        "# Summarization (TensorFlow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-wj4FfYkKOX"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzCT2_r-kKOX",
        "outputId": "101e7668-f385-4126-81be-fcc7248661f2"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.28.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.14.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.13.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.12.0)\n",
            "Requirement already satisfied: protobuf<=3.20.2 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.1.98)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (2.9.2-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]\n",
        "!apt install git-lfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW2G-AJgkKOZ"
      },
      "source": [
        "You will need to setup git, adapt your email and name in the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CTLWhOdakKOa"
      },
      "outputs": [],
      "source": [
        "!git config --global user.email \"jovianjaison@gmail.com\"\n",
        "!git config --global user.name \"jovianjaison\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDjRIVpCkKOb"
      },
      "source": [
        "You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34,
          "referenced_widgets": [
            "46a22d900ab44251ad24408ffa75f28d"
          ]
        },
        "id": "3UXNzEVFkKOc",
        "outputId": "2291d645-9e75-4469-8a15-db6f1831072a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46a22d900ab44251ad24408ffa75f28d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367,
          "referenced_widgets": [
            "ac357443814a4580a83daf12adcd98a5"
          ]
        },
        "id": "w4oH__u4kKOd",
        "outputId": "eef9c7ad-c63e-466b-ca7c-4787f2491da9"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.builder:Found cached dataset amazon_reviews_multi (/root/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac357443814a4580a83daf12adcd98a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
              "        num_rows: 200000\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
              "        num_rows: 5000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
              "        num_rows: 5000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "english_dataset = load_dataset(\"amazon_reviews_multi\", \"en\")\n",
        "english_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhO7lJQUkKOe",
        "outputId": "11ec980a-ae84-4a28-f6dc-630840931e6f"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-0b126a62dc97ecba.arrow\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "'>> Title: Worked in front position, not rear'\n",
            "'>> Review: 3 stars because these are not rear brakes as stated in the item description. At least the mount adapter only worked on the front fork of the bike that I got it for.'\n",
            "\n",
            "'>> Title: meh'\n",
            "'>> Review: Does it’s job and it’s gorgeous but mine is falling apart, I had to basically put it together again with hot glue'\n",
            "\n",
            "'>> Title: Can't beat these for the money'\n",
            "'>> Review: Bought this for handling miscellaneous aircraft parts and hanger \"stuff\" that I needed to organize; it really fit the bill. The unit arrived quickly, was well packaged and arrived intact (always a good sign). There are five wall mounts-- three on the top and two on the bottom. I wanted to mount it on the wall, so all I had to do was to remove the top two layers of plastic drawers, as well as the bottom corner drawers, place it when I wanted and mark it; I then used some of the new plastic screw in wall anchors (the 50 pound variety) and it easily mounted to the wall. Some have remarked that they wanted dividers for the drawers, and that they made those. Good idea. My application was that I needed something that I can see the contents at about eye level, so I wanted the fuller-sized drawers. I also like that these are the new plastic that doesn't get brittle and split like my older plastic drawers did. I like the all-plastic construction. It's heavy duty enough to hold metal parts, but being made of plastic it's not as heavy as a metal frame, so you can easily mount it to the wall and still load it up with heavy stuff, or light stuff. No problem there. For the money, you can't beat it. Best one of these I've bought to date-- and I've been using some version of these for over forty years.'\n"
          ]
        }
      ],
      "source": [
        "def show_samples(dataset, num_samples=3, seed=42):\n",
        "    sample = dataset[\"train\"].shuffle(seed=seed).select(range(num_samples))\n",
        "    for example in sample:\n",
        "        print(f\"\\n'>> Title: {example['review_title']}'\")\n",
        "        print(f\"'>> Review: {example['review_body']}'\")\n",
        "\n",
        "\n",
        "show_samples(english_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDaL25PrkKOf",
        "outputId": "e01dfbdc-bb3e-4a14-a8eb-4d33fae0174b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "home                      17679\n",
              "apparel                   15951\n",
              "wireless                  15717\n",
              "other                     13418\n",
              "beauty                    12091\n",
              "drugstore                 11730\n",
              "kitchen                   10382\n",
              "toy                        8745\n",
              "sports                     8277\n",
              "automotive                 7506\n",
              "lawn_and_garden            7327\n",
              "home_improvement           7136\n",
              "pet_products               7082\n",
              "digital_ebook_purchase     6749\n",
              "pc                         6401\n",
              "electronics                6186\n",
              "office_product             5521\n",
              "shoes                      5197\n",
              "grocery                    4730\n",
              "book                       3756\n",
              "Name: product_category, dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "english_dataset.set_format(\"pandas\")\n",
        "english_df = english_dataset[\"train\"][:]\n",
        "# Show counts for top 20 products\n",
        "english_df[\"product_category\"].value_counts()[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "H-FoWGgMkKOf"
      },
      "outputs": [],
      "source": [
        "def filter_books(example):\n",
        "    return (\n",
        "        example[\"product_category\"] == \"book\"\n",
        "        or example[\"product_category\"] == \"digital_ebook_purchase\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-ycuWkCEkKOf"
      },
      "outputs": [],
      "source": [
        "english_dataset.reset_format()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFZJ0ELQkKOf",
        "outputId": "5fc6823e-9905-4fb4-96a8-716b7a928484"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-850cccae20e66d85.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-2269bd0a5cf796de.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-5a6ac670b2564c86.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-4b2efc3bb6118c77.arrow\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "'>> Title: I'm dissapointed.'\n",
            "'>> Review: I guess I had higher expectations for this book from the reviews. I really thought I'd at least like it. The plot idea was great. I loved Ash but, it just didnt go anywhere. Most of the book was about their radio show and talking to callers. I wanted the author to dig deeper so we could really get to know the characters. All we know about Grace is that she is attractive looking, Latino and is kind of a brat. I'm dissapointed.'\n",
            "\n",
            "'>> Title: Good art, good price, poor design'\n",
            "'>> Review: I had gotten the DC Vintage calendar the past two years, but it was on backorder forever this year and I saw they had shrunk the dimensions for no good reason. This one has good art choices but the design has the fold going through the picture, so it's less aesthetically pleasing, especially if you want to keep a picture to hang. For the price, a good calendar'\n",
            "\n",
            "'>> Title: Helpful'\n",
            "'>> Review: Nearly all the tips useful and. I consider myself an intermediate to advanced user of OneNote. I would highly recommend.'\n"
          ]
        }
      ],
      "source": [
        "english_books = english_dataset.filter(filter_books)\n",
        "show_samples(english_books)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kj0ZK_STkKOg",
        "outputId": "bb4544c2-1246-45a6-f2a2-1df2349b40e3"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-4b2efc3bb6118c77.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-38aeab7931f01bba.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-8c28a3d0e63090a8.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-b1a911e9dcec448f.arrow\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "'>> Title: Never showed up'\n",
            "'>> Review: Ordered it on a friday and it never came. I waited until the following Wednesday and still nothing. :( plus I can't get a refund because I have nothing TO SEND BACK BECAUSE IT NEVER SHOWED UP'\n",
            "\n",
            "'>> Title: Trite. Patronizing. Shallow.'\n",
            "'>> Review: Do yourself a favor and check it out from the library before buying this book. Wish I had. Trust your gut instinct. That's it. Nothing more.'\n",
            "\n",
            "'>> Title: We ordered this book back in Sept and never received ...'\n",
            "'>> Review: We ordered this book back in Sept and never received it. There is now no tracking information or any way to get ahold of the seller.'\n"
          ]
        }
      ],
      "source": [
        "from datasets import concatenate_datasets, DatasetDict\n",
        "\n",
        "books_dataset = DatasetDict()\n",
        "\n",
        "for split in english_books.keys():\n",
        "    books_dataset[split] = english_books[split]\n",
        "    \n",
        "    books_dataset[split] = books_dataset[split].shuffle(seed=42)\n",
        "\n",
        "# Peek at a few examples\n",
        "show_samples(books_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lawIJ9BJkKOg",
        "outputId": "a9b5205d-c614-4e97-8663-a03d9b4dc81c"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-ed5e567059d604ac.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-f4347498d6ba3c1a.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-b96e6f2c0c6c6777.arrow\n"
          ]
        }
      ],
      "source": [
        "books_dataset = books_dataset.filter(lambda x: len(x[\"review_title\"].split()) > 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "F9rb_oVHkKOh"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "#model_checkpoint = \"google/mt5-small\"\n",
        "model_checkpoint=\"jovianjaison/mt5-small-finetuned-amazon-en-es\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MW5BWE9hkKOh",
        "outputId": "41f4a141-0ef2-4ec2-9e4a-cb62e111c346"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [336, 259, 28387, 11807, 287, 62893, 295, 12507, 309, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = tokenizer(\"I loved reading the Hunger Games!\")\n",
        "inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMWHEzFzkKOi",
        "outputId": "527b0901-e676-4d43-fe4c-297ce86f0c72"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['▁I', '▁', 'loved', '▁reading', '▁the', '▁Hung', 'er', '▁Games', '!', '</s>']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.convert_ids_to_tokens(inputs.input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "f-1kl51mkKOi"
      },
      "outputs": [],
      "source": [
        "max_input_length = 512\n",
        "max_target_length = 30\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    model_inputs = tokenizer(\n",
        "        examples[\"review_body\"],\n",
        "        max_length=max_input_length,\n",
        "        truncation=True,\n",
        "    )\n",
        "    labels = tokenizer(\n",
        "        examples[\"review_title\"], max_length=max_target_length, truncation=True\n",
        "    )\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69,
          "referenced_widgets": [
            "4ebad79d5ad847c0b8d04ce7f3802b9e",
            "3f14ec4c352640e9a00d8c3012a0acbc",
            "4254790da6d64eb3bb240a8cd938f787"
          ]
        },
        "id": "I4YMjvIvkKOi",
        "outputId": "15f42c0c-f782-4ae0-c2e3-433ec145e67f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ebad79d5ad847c0b8d04ce7f3802b9e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/6162 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3f14ec4c352640e9a00d8c3012a0acbc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/135 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4254790da6d64eb3bb240a8cd938f787",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/164 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenized_datasets = books_dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "PbcFQgFxkKOi"
      },
      "outputs": [],
      "source": [
        "generated_summary = \"I absolutely loved reading the Hunger Games\"\n",
        "reference_summary = \"I loved reading the Hunger Games\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FohJB5JTkKOi",
        "outputId": "92b51733-7165-4cea-e840-7cb4dc2edd16"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.22.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.65.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2022.10.31)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "wpz53sijkKOj"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "rouge_score = evaluate.load(\"rouge\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mmxo3u-8kKOj",
        "outputId": "171fc87a-b7a0-4175-e50c-72fa9f9d720e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'rouge1': 0.923076923076923,\n",
              " 'rouge2': 0.7272727272727272,\n",
              " 'rougeL': 0.923076923076923,\n",
              " 'rougeLsum': 0.923076923076923}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scores = rouge_score.compute(\n",
        "    predictions=[generated_summary], references=[reference_summary]\n",
        ")\n",
        "scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNZ0dZV-kKOj",
        "outputId": "f83a5ea9-53b7-49fa-d671-789e73d6be20"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.923076923076923"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scores[\"rouge1\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqEgLgoKkKOj",
        "outputId": "10f05aa6-967d-41e7-83f4-0e7cd774d1c9"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkk_qYbNkKOj",
        "outputId": "eecc835e-e59f-4e48-8f87-836b8b898e66"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxYUioFRkKOj",
        "outputId": "c5c75b7a-d518-4b03-9bd3-5657bd6984dc"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Incorrect book mailed.\n",
            "I’m on a time constraint with school and now I have the wrong book with less time to get the correct one.\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "\n",
        "def three_sentence_summary(text):\n",
        "    return \"\\n\".join(sent_tokenize(text)[:3])\n",
        "\n",
        "\n",
        "print(three_sentence_summary(books_dataset[\"train\"][1][\"review_body\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Bm0T0XSskKOj"
      },
      "outputs": [],
      "source": [
        "def evaluate_baseline(dataset, metric):\n",
        "    summaries = [three_sentence_summary(text) for text in dataset[\"review_body\"]]\n",
        "    return metric.compute(predictions=summaries, references=dataset[\"review_title\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kx0uES-RkKOj",
        "outputId": "660691f0-2f40-4041-ec66-b21d33535372"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'rouge1': 15.25, 'rouge2': 8.56, 'rougeL': 14.29, 'rougeLsum': 14.85}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "score = evaluate_baseline(books_dataset[\"validation\"], rouge_score)\n",
        "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
        "rouge_dict = dict((rn, round(score[rn] * 100, 2)) for rn in rouge_names)\n",
        "rouge_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dALr2Z8GkKOj",
        "outputId": "25e42fbf-5235-41b6-f332-001019f70cc4"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
            "  warnings.warn(\n",
            "All model checkpoint layers were used when initializing TFMT5ForConditionalGeneration.\n",
            "\n",
            "All the layers of TFMT5ForConditionalGeneration were initialized from the model checkpoint at jovianjaison/mt5-small-finetuned-amazon-en-es.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMT5ForConditionalGeneration for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFAutoModelForSeq2SeqLM\n",
        "\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34,
          "referenced_widgets": [
            "4e2323b8136a4cb997e9a8d357045269"
          ]
        },
        "id": "ByjaB6-3kKOj",
        "outputId": "30c76de7-e818-42a5-e6c6-dcf951dc8dd3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4e2323b8136a4cb997e9a8d357045269",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "O5az7vwIkKOj"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "EzZW1LVgkKOj"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = tokenized_datasets.remove_columns(\n",
        "    books_dataset[\"train\"].column_names\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAcBrGPFkKOj",
        "outputId": "f03a482f-acc9-4f65-f1c7-7b0e0e28ce87"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input_ids': <tf.Tensor: shape=(2, 90), dtype=int32, numpy=\n",
              "array([[   336,   1425,  34042,    278,    287,  12782,  22417,  53765,\n",
              "           287,   9002,   2956,   3127,    261,   1156,    609,    639,\n",
              "           351,   3004,  11578,  98661,    714,   3721,    305,    336,\n",
              "         19820,    287,    276,   1425,   8124,  83574,    287,    259,\n",
              "         98978,    332,    375,   2316,  10870,    260,   1494,   1371,\n",
              "          1070,   2316,   3684,  18201,    263,   1156,    287,   1719,\n",
              "          1070,    287,  84338,    259,   5846,   3026,    287,  18784,\n",
              "           261,    510,    609,    277,    263,  24691,    259, 143536,\n",
              "          4621,    421,  98991,    261,    259,  21230,    955,    521,\n",
              "          3007,    288,   9158,    259,    262,  18784,    288,  11446,\n",
              "           260,   1102,    287,   6396,    261,    259,    262,   2316,\n",
              "         53765,      1],\n",
              "       [   563,  64777,   3435,  14945,    345,    260,    336,    293,\n",
              "           282,    351,    259,    262,   1459,    317,  98945,    514,\n",
              "          5560,    305,   2561,    336,    783,    287,  31739,   3435,\n",
              "           514,  24691,   1459,    288,   1689,    287,  12382,   1371,\n",
              "           260,      1,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 90), dtype=int32, numpy=\n",
              "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0]], dtype=int32)>, 'labels': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=\n",
              "array([[ 8204,  3684,   261,  2316,  6396,   261, 44960,  1719,     1],\n",
              "       [  564, 23301,  3435,  8407,     1,  -100,  -100,  -100,  -100]],\n",
              "      dtype=int32)>, 'decoder_input_ids': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=\n",
              "array([[    0,  8204,  3684,   261,  2316,  6396,   261, 44960,  1719],\n",
              "       [    0,   564, 23301,  3435,  8407,     1,     0,     0,     0]],\n",
              "      dtype=int32)>}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "features = [tokenized_datasets[\"train\"][i] for i in range(2)]\n",
        "data_collator(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pn67cdrekKOk",
        "outputId": "d6302da7-e36e-4634-b2a2-c812273fd829"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:388: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
            "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
            "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
            "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
            "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tf_train_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_datasets[\"train\"],\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=True,\n",
        "    batch_size=8,\n",
        ")\n",
        "tf_eval_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_datasets[\"validation\"],\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=False,\n",
        "    batch_size=8,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ok9frNKnkKOk",
        "outputId": "312c989d-2045-45d4-d58f-ef03d7e3a716"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n",
            "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
            "The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\n",
            "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n"
          ]
        }
      ],
      "source": [
        "from transformers import create_optimizer\n",
        "import tensorflow as tf\n",
        "\n",
        "# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied\n",
        "# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,\n",
        "# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.\n",
        "num_train_epochs = 8\n",
        "num_train_steps = len(tf_train_dataset) * num_train_epochs\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "\n",
        "optimizer, schedule = create_optimizer(\n",
        "    init_lr=5.6e-5,\n",
        "    num_warmup_steps=0,\n",
        "    num_train_steps=num_train_steps,\n",
        "    weight_decay_rate=0.01,\n",
        ")\n",
        "\n",
        "model.compile(optimizer=optimizer)\n",
        "\n",
        "# Train in mixed-precision float16\n",
        "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4j49QZ_ckKOk",
        "outputId": "96ef626b-1934-4ff1-e026-f7cec676fd90"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/jovianjaison/mt5-small-finetuned-amazon-en-es is already a clone of https://huggingface.co/jovianjaison/mt5-small-finetuned-amazon-en-es. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "WARNING:huggingface_hub.repository:/content/jovianjaison/mt5-small-finetuned-amazon-en-es is already a clone of https://huggingface.co/jovianjaison/mt5-small-finetuned-amazon-en-es. Make sure you pull the latest changes with `repo.git_pull()`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "628/770 [=======================>......] - ETA: 29:58 - loss: 4.5864"
          ]
        }
      ],
      "source": [
        "from transformers.keras_callbacks import PushToHubCallback\n",
        "#output_dir=f\"{model_name}-finetuned-amazon-en-es\"\n",
        "callback = PushToHubCallback(\n",
        "    output_dir=\"jovianjaison/mt5-small-finetuned-amazon-en-es\", tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback], epochs=8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B96bXI73kKOk"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "generation_data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer, model=model, return_tensors=\"tf\", pad_to_multiple_of=320\n",
        ")\n",
        "\n",
        "tf_generate_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_datasets[\"validation\"],\n",
        "    collate_fn=generation_data_collator,\n",
        "    shuffle=False,\n",
        "    batch_size=8,\n",
        "    drop_remainder=True,\n",
        ")\n",
        "\n",
        "\n",
        "@tf.function(jit_compile=True)\n",
        "def generate_with_xla(batch):\n",
        "    return model.generate(\n",
        "        input_ids=batch[\"input_ids\"],\n",
        "        attention_mask=batch[\"attention_mask\"],\n",
        "        max_new_tokens=32,\n",
        "    )\n",
        "\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "for batch, labels in tqdm(tf_generate_dataset):\n",
        "    predictions = generate_with_xla(batch)\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    labels = labels.numpy()\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "    all_preds.extend(decoded_preds)\n",
        "    all_labels.extend(decoded_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2c15zzvDkKOk"
      },
      "outputs": [],
      "source": [
        "result = rouge_score.compute(\n",
        "    predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
        ")\n",
        "result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "{k: round(v, 4) for k, v in result.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7UNux_zkKOk"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "hub_model_id = \"huggingface-course/mt5-small-finetuned-amazon-en-es\"\n",
        "summarizer = pipeline(\"summarization\", model=hub_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZDAytOIkKOk"
      },
      "outputs": [],
      "source": [
        "def print_summary(idx):\n",
        "    review = books_dataset[\"test\"][idx][\"review_body\"]\n",
        "    title = books_dataset[\"test\"][idx][\"review_title\"]\n",
        "    summary = summarizer(books_dataset[\"test\"][idx][\"review_body\"])[0][\"summary_text\"]\n",
        "    print(f\"'>>> Review: {review}'\")\n",
        "    print(f\"\\n'>>> Title: {title}'\")\n",
        "    print(f\"\\n'>>> Summary: {summary}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdcAFy4gkKOk"
      },
      "outputs": [],
      "source": [
        "print_summary(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb6MgqVBkKOk"
      },
      "outputs": [],
      "source": [
        "print_summary(0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I like this as a vent as well as something that will keep house warmer in winter.  I sanded it and then painted it the same color as the house.  Looks great.\"\n",
        "summary = summarizer()[0][\"summary_text\"]"
      ],
      "metadata": {
        "id": "KVDwqyjGppBx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Summarization (TensorFlow)",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}